{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 작업준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우\n",
    "import tensorflow as tf\n",
    "\n",
    "# 넘파이\n",
    "import numpy as np\n",
    "\n",
    "# 판다스\n",
    "import pandas as pd\n",
    "\n",
    "# 맷플롯립\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR Gate Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "x_data = np.array([[0, 0, 0], \n",
    "                   [0, 0, 1], \n",
    "                   [0, 1, 0], \n",
    "                   [0, 1, 1], \n",
    "                   [1, 0, 0],\n",
    "                   [1, 1, 0],\n",
    "                   [1, 0, 1], \n",
    "                   [1, 1, 1]], \n",
    "                  dtype=np.float32)\n",
    "                   \n",
    "                   \n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [1]], dtype=np.float32)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), dtype=tf.float32)\n",
    "b = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypot) + (1-y) * tf.log(1-hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype = tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(1000): \n",
    "    sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "    \n",
    "\n",
    "h, p, a = sess.run([hypot, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "\n",
    "\n",
    "print(\"가설 :\", h, \"\\n예측:\", p, \"\\n정확도:\", a)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], \n",
    "                   [0, 0, 1], \n",
    "                   [0, 1, 0], \n",
    "                   [0, 1, 1], \n",
    "                   [1, 0, 0],\n",
    "                   [1, 1, 0],\n",
    "                   [1, 0, 1], \n",
    "                   [1, 1, 1]], \n",
    "                  dtype=np.float32)\n",
    "                   \n",
    "                   \n",
    "y_data = np.array([[0], [0], [0], [0], [0], [0], [0], [1]], dtype=np.float32)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), dtype=tf.float32)\n",
    "b = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypot) + (1-y) * tf.log(1-hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype = tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(1000): \n",
    "    sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "    \n",
    "\n",
    "h, p, a = sess.run([hypot, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "\n",
    "\n",
    "print(\"가설 :\", h, \"\\n예측:\", p, \"\\n정확도:\", a)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                   [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [0], [0], [0], [0], [0], [0], [1]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), dtype=tf.float32)\n",
    "b = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypot) + (1-y) * tf.log(1-hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1000):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "h, p, a = sess.run([hypot, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "print(\"가설:\", h, \"\\n예측:\", p, \"\\n정확도:\", a)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], \n",
    "                   [0, 0, 1], \n",
    "                   [0, 1, 0], \n",
    "                   [0, 1, 1], \n",
    "                   [1, 0, 0],\n",
    "                   [1, 1, 0],\n",
    "                   [1, 0, 1], \n",
    "                   [1, 1, 1]], \n",
    "                  dtype=np.float32)\n",
    "                   \n",
    "                   \n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), dtype=tf.float32)\n",
    "b = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypot) + (1-y) * tf.log(1-hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype = tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(1000): \n",
    "    sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "    \n",
    "\n",
    "h, p, a = sess.run([hypot, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "\n",
    "\n",
    "print(\"가설 :\", h, \"\\n예측:\", p, \"\\n정확도:\", a)\n",
    "\n",
    "# 정확도가 낮아짐을 볼 수 있다. 간단한 명제를 기존의 방법으로는 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], \n",
    "                   [0, 0, 1], \n",
    "                   [0, 1, 0], \n",
    "                   [0, 1, 1], \n",
    "                   [1, 0, 0],\n",
    "                   [1, 1, 0],\n",
    "                   [1, 0, 1], \n",
    "                   [1, 1, 1]], \n",
    "                  dtype=np.float32)\n",
    "                   \n",
    "                   \n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=100)\n",
    "clf.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [[1, 1, 1], [0, 0, 0], [0, 0, 1], [0, 1, 0]]\n",
    "test_label = [0, 0, 1, 1]\n",
    "\n",
    "result = clf.predict(test)\n",
    "print(result)\n",
    "\n",
    "score = metrics.accuracy_score(test_label, result)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning(->Multi Layer Perceptron)을 이용한 XOR 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                   [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [0], [0], [0], [0], [0], [0], [1]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "\n",
    "# TLU 1: n-> 입력값을 가장 처음으로 전달받음\n",
    "W1 = tf.Variable(tf.random_normal([3, 10]), dtype=tf.float32)\n",
    "                                # 전달받는 값의 크기만큼\n",
    "                                      # 다음 히든으로 출력하는 값의 개수 - 전달 할 노드 수 만큼\n",
    "b1 = tf.Variable(tf.random_normal([10]), dtype=tf.float32)\n",
    "                                  # 다음 히든으로 출력하는 값의 개수 - 전달 할 노드 수 만큼\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "\n",
    "# TLU 2\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), dtype=tf.float32)\n",
    "                                  # 앞에서 전달받은 만큼\n",
    "                                      # 최종 출력층이므로 y값을 따라가면 됨.\n",
    "b2 = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
    "hypot = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "                              # 앞에서 계산한 최종 값을 전달 받음 -> layer1\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypot) + (1-y) * tf.log(1-hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype = tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(1000): \n",
    "    sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "    \n",
    "\n",
    "h, p, a = sess.run([hypot, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "\n",
    "\n",
    "print(\"가설 :\", h, \"\\n예측:\", p, \"\\n정확도:\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep & Wide\n",
    "\n",
    " - Deep : 6개의 Hidden Layer 추가\n",
    " - 각 계층의 입출력개수는 50개\n",
    " - 결론 : 더 정교, 더 정확한 결과를 얻었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                   [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [0], [0], [0], [0], [0], [0], [1]], dtype=np.float32)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLU 1: n-> 입력값을 가장 처음으로 전달받음\n",
    "W1 = tf.Variable(tf.random_normal([3, 50]), dtype=tf.float32)\n",
    "b1 = tf.Variable(tf.random_normal([50]), dtype=tf.float32)\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# TLU 2\n",
    "W2 = tf.Variable(tf.random_normal([50, 50]), dtype=tf.float32)\n",
    "b2 = tf.Variable(tf.random_normal([50]), dtype=tf.float32)\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# TLU 3\n",
    "W3 = tf.Variable(tf.random_normal([50, 50]), dtype=tf.float32)\n",
    "b3 = tf.Variable(tf.random_normal([50]), dtype=tf.float32)\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "# TLU 4\n",
    "W4 = tf.Variable(tf.random_normal([50, 50]), dtype=tf.float32)\n",
    "b4 = tf.Variable(tf.random_normal([50]), dtype=tf.float32)\n",
    "layer4 = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "# TLU 5\n",
    "W5 = tf.Variable(tf.random_normal([50, 50]), dtype=tf.float32)\n",
    "b5 = tf.Variable(tf.random_normal([50]), dtype=tf.float32)\n",
    "layer5 = tf.sigmoid(tf.matmul(layer4, W5) + b5)\n",
    "\n",
    "# TLU 6\n",
    "W6 = tf.Variable(tf.random_normal([50, 1]), dtype=tf.float32)\n",
    "b6 = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
    "hypot = tf.sigmoid(tf.matmul(layer5, W6) + b6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypot) + (1-y) * tf.log(1-hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y), dtype=tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "for epoch in range(10000): \n",
    "    sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "    \n",
    "h, p, a = sess.run([hypot, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "\n",
    "\n",
    "print(\"가설 :\", h, \"\\n예측:\", p, \"\\n정확도:\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard \n",
    "- print가 아니라 시각적으로 확인해보자!\n",
    "- 5 steps of using TensorBoard\n",
    "\n",
    "   1) 그룹으로 묶어준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존에 저장되어있는 데이터 삭제. 그렇지 않으면 중복되어서 코드 충돌이 일어남.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1, 0, 0],[1, 1, 0],[1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "X= tf.placeholder(tf.float32,shape=[None,3])\n",
    "y= tf.placeholder(tf.float32,shape=[None,1])    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('layer1'):     # with을 이용해서 그룹으로 묶어준다.  \n",
    "    W1= tf.Variable(tf.random_normal([3,50]),dtype=tf.float32)     \n",
    "    b1= tf.Variable(tf.random_normal([50]),dtype=tf.float32)\n",
    "    layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "with tf.name_scope('layer2'): \n",
    "    W2= tf.Variable(tf.random_normal([3,50]),dtype=tf.float32)     \n",
    "    b2= tf.Variable(tf.random_normal([50]),dtype=tf.float32)\n",
    "    hypot = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "    \n",
    "    \n",
    "    tf.summary.histogram('weight2',W2)\n",
    "    tf.summary.histogram('bias2',b2)\n",
    "    tf.summary.histogram('hypot',hypot)\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('cost'):\n",
    "    cost = -tf.reduce_mean(y*tf.log(hypot)+(1-y)*tf.log(1-hypot))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    \n",
    "    \n",
    "    \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('log_dir2/alpha0_1')    # 경로 주의!\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "\n",
    "for step in range(10000):\n",
    "    _,summary =sess.run([train, merged_summary],feed_dict={X:x_data,y:y_data})\n",
    "    writer.add_summary(summary,global_step=step)\n",
    "    \n",
    "h,p,a = sess.run([hypot,pred,accuracy],feed_dict={X:x_data,y:y_data})    \n",
    "\n",
    "print('가설: ',h, '\\n예측: ',p, '\\n정확도: ',a)\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning_rate=0.1, learning_rate=0.01 동시에 확인\n",
    "\n",
    " - 아래 코드는 learning_rate가 0.01일때의 코드. 위의 코드 (learning_rate=0.1) 한 번 실행 후 아래 코드 실행해보자.\n",
    " - 두 작업파일은 같은 부모 파일 안에 있어야 한다.\n",
    " - 코드 실행 후, cmd창에서 tensorboard --logdir=./log_dir2 로 접속."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존에 저장되어있는 데이터 삭제. 그렇지 않으면 중복되어서 코드 충돌이 일어남.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1, 0, 0],[1, 1, 0],[1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "X= tf.placeholder(tf.float32,shape=[None,3])\n",
    "y= tf.placeholder(tf.float32,shape=[None,1])    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('layer1'):     # with을 이용해서 그룹으로 묶어준다.  \n",
    "    W1= tf.Variable(tf.random_normal([3,50]),dtype=tf.float32)     \n",
    "    b1= tf.Variable(tf.random_normal([50]),dtype=tf.float32)\n",
    "    layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "with tf.name_scope('layer2'): \n",
    "    W2= tf.Variable(tf.random_normal([3,50]),dtype=tf.float32)     \n",
    "    b2= tf.Variable(tf.random_normal([50]),dtype=tf.float32)\n",
    "    hypot = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "    \n",
    "    \n",
    "    tf.summary.histogram('weight2',W2)\n",
    "    tf.summary.histogram('bias2',b2)\n",
    "    tf.summary.histogram('hypot',hypot)\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('cost'):\n",
    "    cost = -tf.reduce_mean(y*tf.log(hypot)+(1-y)*tf.log(1-hypot))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    \n",
    "    \n",
    "    \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('log_dir2/alpha0_01')    # 경로 주의!\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "\n",
    "for step in range(10000):\n",
    "    _,summary =sess.run([train, merged_summary],feed_dict={X:x_data,y:y_data})\n",
    "    writer.add_summary(summary,global_step=step)\n",
    "    \n",
    "h,p,a = sess.run([hypot,pred,accuracy],feed_dict={X:x_data,y:y_data})    \n",
    "\n",
    "print('가설: ',h, '\\n예측: ',p, '\\n정확도: ',a)\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ai_data/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ai_data/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ai_data/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ai_data/MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 데이터 준비\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)\n",
    "mnist = input_data.read_data_sets(\"ai_data/MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([28*28, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c,acc = sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"    accuracy:\", acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오히려 더떨어짐 층을 늘렸는데\n",
    "- 인자들 28*28, 256 고쳐주고,  layer1, layer2 = tf.sigmoid(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#레이어 3개 추가 \n",
    "# 입출력 개수는 256개\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([28*28, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.sigmoid(logit)\n",
    "\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.sigmoid(logit)\n",
    "\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.sigmoid(logit)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c,acc = sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"    accuracy:\", acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이제 시그모이드를 RELU로 바꿀꺼임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#레이어 3개 추가 \n",
    "# 입출력 개수는 256개\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([28*28, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.sigmoid(logit)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c,acc = sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"    accuracy:\", acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자비에 인가 자비어인가 초기화 변수 사용\n",
    "\n",
    "## W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "##                     initailizer =tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable('W1', shape = [784, 256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "\n",
    "W2 = tf.get_variable('W2', shape = [256, 256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.sigmoid(logit)\n",
    "\n",
    "W3 = tf.get_variable('W3', shape = [256, 256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.sigmoid(logit)\n",
    "\n",
    "W4 = tf.get_variable('W4', shape = [256, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, acc = sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch + 1), \"    cost:\", avg_cost,\"    accuracy: \",acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer를 8개로 추가하기\n",
    "# 입출력개수 512 개로 바꾸기\n",
    "# 이거 코드 정리 사이트\n",
    "\n",
    "https://pythonkim.tistory.com/44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: nan     accuracy:  0.105\n",
      "epoch: 2     cost: nan     accuracy:  0.095\n",
      "epoch: 3     cost: nan     accuracy:  0.08\n",
      "epoch: 4     cost: nan     accuracy:  0.08\n",
      "epoch: 5     cost: nan     accuracy:  0.125\n",
      "epoch: 6     cost: nan     accuracy:  0.15\n",
      "epoch: 7     cost: nan     accuracy:  0.11\n",
      "epoch: 8     cost: nan     accuracy:  0.075\n",
      "epoch: 9     cost: nan     accuracy:  0.09\n",
      "epoch: 10     cost: nan     accuracy:  0.105\n",
      "epoch: 11     cost: nan     accuracy:  0.09\n",
      "epoch: 12     cost: nan     accuracy:  0.105\n",
      "epoch: 13     cost: nan     accuracy:  0.075\n",
      "epoch: 14     cost: nan     accuracy:  0.11\n",
      "epoch: 15     cost: nan     accuracy:  0.045\n",
      "epoch: 16     cost: nan     accuracy:  0.075\n",
      "epoch: 17     cost: nan     accuracy:  0.1\n",
      "epoch: 18     cost: nan     accuracy:  0.105\n",
      "epoch: 19     cost: nan     accuracy:  0.075\n",
      "epoch: 20     cost: nan     accuracy:  0.07\n",
      "epoch: 21     cost: nan     accuracy:  0.115\n",
      "epoch: 22     cost: nan     accuracy:  0.095\n",
      "epoch: 23     cost: nan     accuracy:  0.135\n",
      "epoch: 24     cost: nan     accuracy:  0.115\n",
      "epoch: 25     cost: nan     accuracy:  0.09\n",
      "epoch: 26     cost: nan     accuracy:  0.105\n",
      "epoch: 27     cost: nan     accuracy:  0.135\n",
      "epoch: 28     cost: nan     accuracy:  0.12\n",
      "epoch: 29     cost: nan     accuracy:  0.105\n",
      "epoch: 30     cost: nan     accuracy:  0.125\n",
      "훈련 종료\n",
      "정확도 :  0.098\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable('W1', shape = [784, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "\n",
    "W2 = tf.get_variable('W2', shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "W3 = tf.get_variable('W3', shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "\n",
    "W4 = tf.get_variable('W4', shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.nn.relu(logit)\n",
    "\n",
    "W5 = tf.get_variable('W5', shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer4, W5) + b5\n",
    "layer5 =tf.nn.relu(logit)\n",
    "\n",
    "W6 = tf.get_variable('W6', shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer5, W6) + b6\n",
    "layer6 =tf.nn.relu(logit)\n",
    "\n",
    "W7 = tf.get_variable('W7', shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "layer7 =tf.nn.relu(logit)\n",
    "\n",
    "W8 = tf.get_variable('W8', shape = [512, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer7, W8) + b8\n",
    "\n",
    "\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, acc = sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch + 1), \"    cost:\", avg_cost,\"    accuracy: \",acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 2.091979304877193     accuracy: 0.31\n",
      "epoch: 2     cost: 0.7112983546473759     accuracy: 0.865\n",
      "epoch: 3     cost: 0.31706547585400674     accuracy: 0.925\n",
      "epoch: 4     cost: 0.2107493607293477     accuracy: 0.935\n",
      "epoch: 5     cost: 0.15854648464105348     accuracy: 0.945\n",
      "epoch: 6     cost: 0.12435244116593491     accuracy: 0.985\n",
      "epoch: 7     cost: 0.10233750940046529     accuracy: 0.97\n",
      "epoch: 8     cost: 0.08699808781797229     accuracy: 0.98\n",
      "epoch: 9     cost: 0.0771655390953476     accuracy: 0.99\n",
      "epoch: 10     cost: 0.059822500270198684     accuracy: 0.975\n",
      "epoch: 11     cost: 0.05255367509682067     accuracy: 0.975\n",
      "epoch: 12     cost: 0.04380816715515472     accuracy: 0.995\n",
      "epoch: 13     cost: 0.038640475130894     accuracy: 0.99\n",
      "epoch: 14     cost: 0.03844084111465651     accuracy: 0.985\n",
      "epoch: 15     cost: 0.02633815706707537     accuracy: 0.99\n",
      "epoch: 16     cost: 0.02247126275123182     accuracy: 0.995\n",
      "epoch: 17     cost: 0.02075871639631011     accuracy: 1.0\n",
      "epoch: 18     cost: 0.26912186553511275     accuracy: 0.99\n",
      "epoch: 19     cost: 0.04914481819353319     accuracy: 0.995\n",
      "epoch: 20     cost: 0.02459467259743675     accuracy: 1.0\n",
      "epoch: 21     cost: 0.01569625445641577     accuracy: 1.0\n",
      "epoch: 22     cost: 0.011431652591140434     accuracy: 1.0\n",
      "epoch: 23     cost: 0.009605088963236823     accuracy: 1.0\n",
      "epoch: 24     cost: 0.009324854929846797     accuracy: 0.98\n",
      "epoch: 25     cost: 0.0066691782050342695     accuracy: 0.995\n",
      "epoch: 26     cost: 0.003641645464284178     accuracy: 1.0\n",
      "epoch: 27     cost: 0.0031111145377243775     accuracy: 1.0\n",
      "epoch: 28     cost: 0.002161681504750794     accuracy: 0.995\n",
      "epoch: 29     cost: 0.0015200213764116851     accuracy: 1.0\n",
      "epoch: 30     cost: 0.0013195994819133448     accuracy: 1.0\n",
      "훈련 종료\n",
      "정확도 :  0.982\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[28*28, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.nn.relu(logit)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.nn.relu(logit)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.nn.relu(logit)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer6, W7) + b7\n",
    "layer7 = tf.sigmoid(logit)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit= tf.matmul(layer7, W8) + b8\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c,acc = sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"    accuracy:\", acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout : 오버피팅으로부터 간단하게 방지하는 방법(과적합)\n",
    "##  sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys, prob:0.7}) 먹이넣는 곳에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-08ebe0f1ee8b>:15: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "epoch: 1     cost: 2.2709549821506836     accuracy: 0.29\n",
      "epoch: 2     cost: 1.2276980803229596     accuracy: 0.765\n",
      "epoch: 3     cost: 0.584875634908676     accuracy: 0.875\n",
      "epoch: 4     cost: 0.37959107100963574     accuracy: 0.915\n",
      "epoch: 5     cost: 0.29300074772401297     accuracy: 0.945\n",
      "epoch: 6     cost: 0.24214207803661184     accuracy: 0.935\n",
      "epoch: 7     cost: 0.21249894228848545     accuracy: 0.96\n",
      "epoch: 8     cost: 0.18795408495447846     accuracy: 0.935\n",
      "epoch: 9     cost: 0.1665444232252511     accuracy: 0.945\n",
      "epoch: 10     cost: 0.15230438478968353     accuracy: 0.945\n",
      "epoch: 11     cost: 0.13906951625238764     accuracy: 0.95\n",
      "epoch: 12     cost: 0.1304927491870794     accuracy: 0.965\n",
      "epoch: 13     cost: 0.12306476538154212     accuracy: 0.98\n",
      "epoch: 14     cost: 0.1119930722090331     accuracy: 0.985\n",
      "epoch: 15     cost: 0.10592722900889133     accuracy: 0.965\n",
      "epoch: 16     cost: 0.10124950503761128     accuracy: 0.975\n",
      "epoch: 17     cost: 0.09276616720313366     accuracy: 0.995\n",
      "epoch: 18     cost: 0.08674665015868163     accuracy: 0.97\n",
      "epoch: 19     cost: 0.0856826327334751     accuracy: 0.97\n",
      "epoch: 20     cost: 0.07949397551403803     accuracy: 0.975\n",
      "epoch: 21     cost: 0.07483037949963052     accuracy: 0.965\n",
      "epoch: 22     cost: 0.07444242900406772     accuracy: 0.96\n",
      "epoch: 23     cost: 0.06935982713645157     accuracy: 0.98\n",
      "epoch: 24     cost: 0.0705341100963679     accuracy: 0.965\n",
      "epoch: 25     cost: 0.0652222932807424     accuracy: 0.975\n",
      "epoch: 26     cost: 0.06518066716465082     accuracy: 0.99\n",
      "epoch: 27     cost: 0.06153042895888745     accuracy: 0.965\n",
      "epoch: 28     cost: 0.05832978058437057     accuracy: 0.985\n",
      "epoch: 29     cost: 0.05443772464821288     accuracy: 0.985\n",
      "epoch: 30     cost: 0.05429695007645273     accuracy: 0.965\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[28*28, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "layer1 = tf.nn.dropout(layer1, keep_prob = prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "layer2 = tf.nn.dropout(layer2, keep_prob = prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "layer3 = tf.nn.dropout(layer3, keep_prob = prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.nn.relu(logit)\n",
    "layer4 = tf.nn.dropout(layer4, keep_prob = prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.nn.relu(logit)\n",
    "layer5 = tf.nn.dropout(layer5, keep_prob = prob)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.nn.relu(logit)\n",
    "layer6 = tf.nn.dropout(layer6, keep_prob = prob)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer6, W7) + b7\n",
    "layer7 = tf.sigmoid(logit)\n",
    "layer7 = tf.nn.dropout(layer7, keep_prob = prob)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit= tf.matmul(layer7, W8) + b8\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c,acc = sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys, prob:0.7})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"    accuracy:\", acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prob : 1 로 조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9799\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels, prob:1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미분공식을 Adam으로 바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 0.7818947635455569     accuracy: 0.935\n",
      "epoch: 2     cost: 0.22606975392861803     accuracy: 0.955\n",
      "epoch: 3     cost: 0.16738709650256417     accuracy: 0.95\n",
      "epoch: 4     cost: 0.1373105219954795     accuracy: 0.975\n",
      "epoch: 5     cost: 0.11784685926003895     accuracy: 0.965\n",
      "epoch: 6     cost: 0.10605510549111807     accuracy: 0.98\n",
      "epoch: 7     cost: 0.09373098588802604     accuracy: 0.985\n",
      "epoch: 8     cost: 0.08635694897987625     accuracy: 0.99\n",
      "epoch: 9     cost: 0.08445858967236497     accuracy: 0.955\n",
      "epoch: 10     cost: 0.07697372143241485     accuracy: 0.98\n",
      "epoch: 11     cost: 0.07449017460711982     accuracy: 0.99\n",
      "epoch: 12     cost: 0.06856400955468413     accuracy: 0.99\n",
      "epoch: 13     cost: 0.07161104618825696     accuracy: 0.985\n",
      "epoch: 14     cost: 0.07169256610457193     accuracy: 0.99\n",
      "epoch: 15     cost: 0.0675217073461549     accuracy: 0.99\n",
      "epoch: 16     cost: 0.0613522620600733     accuracy: 0.99\n",
      "epoch: 17     cost: 0.066058878895234     accuracy: 0.98\n",
      "epoch: 18     cost: 0.06507739525627006     accuracy: 0.975\n",
      "epoch: 19     cost: 0.07958272557536301     accuracy: 0.975\n",
      "epoch: 20     cost: 0.07426304090429439     accuracy: 0.99\n",
      "epoch: 21     cost: 0.07503758050162683     accuracy: 0.995\n",
      "epoch: 22     cost: 0.08382249659435316     accuracy: 0.965\n",
      "epoch: 23     cost: 0.09048953580246731     accuracy: 0.99\n",
      "epoch: 24     cost: 0.0910381228171966     accuracy: 0.95\n",
      "epoch: 25     cost: 0.10425908002325075     accuracy: 0.98\n",
      "epoch: 26     cost: 0.11532318932427602     accuracy: 0.965\n",
      "epoch: 27     cost: 0.12577940592711617     accuracy: 0.945\n",
      "epoch: 28     cost: 0.13985408012839878     accuracy: 0.97\n",
      "epoch: 29     cost: 0.18492538872090253     accuracy: 0.965\n",
      "epoch: 30     cost: 0.18374620012261636     accuracy: 0.955\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[28*28, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "layer1 = tf.nn.dropout(layer1, keep_prob = prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "layer2 = tf.nn.dropout(layer2, keep_prob = prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "layer3 = tf.nn.dropout(layer3, keep_prob = prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.nn.relu(logit)\n",
    "layer4 = tf.nn.dropout(layer4, keep_prob = prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.nn.relu(logit)\n",
    "layer5 = tf.nn.dropout(layer5, keep_prob = prob)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.nn.relu(logit)\n",
    "layer6 = tf.nn.dropout(layer6, keep_prob = prob)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit= tf.matmul(layer6, W7) + b7\n",
    "layer7 = tf.sigmoid(logit)\n",
    "layer7 = tf.nn.dropout(layer7, keep_prob = prob)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit= tf.matmul(layer7, W8) + b8\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c,acc = sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys, prob:0.7})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"    accuracy:\", acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9657\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels, prob:1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers : AdamOptimizer가 가장 무난하다\n",
    "\n",
    "## 그외 존나게 많다\n",
    " tf.train.AdadeltaOptimizer\n",
    " tf.train.AdagradOptimizer\n",
    " tf.train.AdagradDAOptimizer\n",
    " tf.train.MomentumOptimizer\n",
    " tf.train.AdamOptimizer\n",
    " tf.train.FtrlOptimizer\n",
    " tf.train.ProximalGradientDescentOptimizer\n",
    " tf.train.ProximalAdagradOptimizer\n",
    " tf.train.RMSPropOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
